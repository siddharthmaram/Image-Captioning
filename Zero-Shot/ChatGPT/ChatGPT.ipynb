{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:20:21.129948Z",
     "iopub.status.busy": "2025-10-30T06:20:21.129693Z",
     "iopub.status.idle": "2025-10-30T06:21:40.606105Z",
     "shell.execute_reply": "2025-10-30T06:21:40.605172Z",
     "shell.execute_reply.started": "2025-10-30T06:20:21.129927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install openai bert-score rouge-score pycocoevalcap nltk pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:40.608038Z",
     "iopub.status.busy": "2025-10-30T06:21:40.607800Z",
     "iopub.status.idle": "2025-10-30T06:21:51.930386Z",
     "shell.execute_reply": "2025-10-30T06:21:51.929768Z",
     "shell.execute_reply.started": "2025-10-30T06:21:40.608017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# OpenAI client (Responses API)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Metrics\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Ensure basic NLTK resources (BLEU doesn't need tokenizers, but good practice)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:51.931571Z",
     "iopub.status.busy": "2025-10-30T06:21:51.931102Z",
     "iopub.status.idle": "2025-10-30T06:21:51.935891Z",
     "shell.execute_reply": "2025-10-30T06:21:51.935193Z",
     "shell.execute_reply.started": "2025-10-30T06:21:51.931552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/split-10k-dataset/Split Dataset/test\"\n",
    "CSV_PATH = f\"{DATA_DIR}/description_b.csv\"\n",
    "IMAGES_BASE = f\"{DATA_DIR}\"\n",
    "\n",
    "# Model and prompt\n",
    "MODEL_NAME = \"gpt-5\"  # change to the exact GPT-5 model available on your account if needed\n",
    "CAPTION_PROMPT = (\n",
    "    '''Strictly compose a single, small, concise paragraph of 3 to 4 lines describing this labeled biological diagram. \n",
    "    Identify all key biological structures, organelles, molecules, or entities indicated by the labels \n",
    "    or annotations. Explicitly reference these labels and explain their spatial relationships, \n",
    "    interactions, and functions with precise biological terminology. Ensure the description is succinct, \n",
    "    informative, and scientifically accurate, without extra elaboration or multiple paragraphs.'''\n",
    ")\n",
    "\n",
    "# Limit to 5 samples to save cost; set to None to run all\n",
    "NUM_SAMPLES = None\n",
    "\n",
    "# Outputs\n",
    "PREDICTIONS_CSV = \"predictions.csv\"\n",
    "SCORES_JSON = \"scores.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:51.936895Z",
     "iopub.status.busy": "2025-10-30T06:21:51.936666Z",
     "iopub.status.idle": "2025-10-30T06:21:52.189376Z",
     "shell.execute_reply": "2025-10-30T06:21:52.188797Z",
     "shell.execute_reply.started": "2025-10-30T06:21:51.936880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "except Exception:\n",
    "    # Fallback to environment variable set manually in Settings -> Environment\n",
    "    pass\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:52.191251Z",
     "iopub.status.busy": "2025-10-30T06:21:52.191058Z",
     "iopub.status.idle": "2025-10-30T06:21:52.247213Z",
     "shell.execute_reply": "2025-10-30T06:21:52.246528Z",
     "shell.execute_reply.started": "2025-10-30T06:21:52.191236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"file_name\", \"text\"}.issubset(df.columns), \"CSV must have 'file_name' and 'text' columns.\"\n",
    "\n",
    "if NUM_SAMPLES is not None:\n",
    "    # Deterministic random sample of 5 to minimize cost\n",
    "    df = df.sample(n=int(NUM_SAMPLES), random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:52.248018Z",
     "iopub.status.busy": "2025-10-30T06:21:52.247839Z",
     "iopub.status.idle": "2025-10-30T06:21:52.256489Z",
     "shell.execute_reply": "2025-10-30T06:21:52.255797Z",
     "shell.execute_reply.started": "2025-10-30T06:21:52.248004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def func(n):\n",
    "    return n.split('.')[-1]\n",
    "\n",
    "set(map(func,df['file_name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:21:52.257245Z",
     "iopub.status.busy": "2025-10-30T06:21:52.257081Z",
     "iopub.status.idle": "2025-10-30T06:21:52.269341Z",
     "shell.execute_reply": "2025-10-30T06:21:52.268634Z",
     "shell.execute_reply.started": "2025-10-30T06:21:52.257232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "def encode_image_as_data_url(image_path: str) -> str:\n",
    "    ext = Path(image_path).suffix.lower()\n",
    "    mime = {\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.png': 'image/png',\n",
    "        '.bmp': 'image/bmp',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }.get(ext, 'image/jpeg')\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    b64_str = base64.b64encode(img_bytes).decode('utf-8')\n",
    "    data_url = f\"data:{mime};base64,{b64_str}\"\n",
    "    return data_url\n",
    "\n",
    "def extract_text_from_response(response) -> str:\n",
    "    # Find the first output item with 'content' attribute (the message)\n",
    "    for item in response.output:\n",
    "        if hasattr(item, \"content\"):\n",
    "            # content is a list of blocks, find first text block\n",
    "            for block in item.content:\n",
    "                if hasattr(block, \"type\") and block.type in (\"output_text\", \"text\"):\n",
    "                    return getattr(block, \"text\", \"\").strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_with_openai(image_path: str, prompt: str) -> str:\n",
    "    data_url = encode_image_as_data_url(image_path)\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL_NAME,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": prompt},\n",
    "                    {\"type\": \"input_image\", \"image_url\": data_url},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_output_tokens=2000\n",
    "    )\n",
    "\n",
    "    # print(resp)\n",
    "    txt = extract_text_from_response(resp)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-30T10:11:42.238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "PREDICTIONS_CSV = \"predictions.csv\"\n",
    "PREDICTIONS_JSONL = \"predictions.jsonl\"  # one JSON object per line\n",
    "\n",
    "# Build a skip set from existing JSONL (if present)\n",
    "processed = set()\n",
    "if os.path.exists(PREDICTIONS_JSONL):\n",
    "    with open(PREDICTIONS_JSONL, \"r\", encoding=\"utf-8\") as rj:\n",
    "        for line in rj:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                processed.add(obj.get(\"image_id\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "records = []\n",
    "with open(PREDICTIONS_JSONL, \"a\", encoding=\"utf-8\") as jf:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Captioning\"):\n",
    "        rel_path = row[\"file_name\"]\n",
    "        if rel_path in processed:\n",
    "            continue  # already done\n",
    "            \n",
    "        rel_path = row[\"file_name\"]\n",
    "        ref_text = str(row[\"text\"]).strip()\n",
    "        img_path = os.path.join(IMAGES_BASE, rel_path)\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path_alt = os.path.join(DATA_DIR, rel_path)\n",
    "            if os.path.exists(img_path_alt):\n",
    "                img_path = img_path_alt\n",
    "\n",
    "        pred_text = generate_caption_with_openai(img_path, CAPTION_PROMPT)\n",
    "        # print(\"Pred:\", pred_text)\n",
    "\n",
    "        rec = {\n",
    "            \"image_id\": rel_path,      # keep a stable key name\n",
    "            \"pred\": pred_text,         # the model output\n",
    "            \"reference\": ref_text,     # optional: keep reference too\n",
    "            \"ts\": time.time(),         # optional: for debugging/resume\n",
    "        }\n",
    "        records.append({\n",
    "            \"file_name\": rel_path,\n",
    "            \"reference\": ref_text,\n",
    "            \"prediction\": pred_text,\n",
    "        })\n",
    "\n",
    "        # Stream to JSONL immediately\n",
    "        jf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        jf.flush()  # force write to disk frequently\n",
    "\n",
    "# After loop, save/overwrite the CSV snapshot\n",
    "pred_df = pd.DataFrame(records)\n",
    "pred_df.to_csv(PREDICTIONS_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T00:09:05.724451Z",
     "iopub.status.idle": "2025-10-18T00:09:05.724658Z",
     "shell.execute_reply": "2025-10-18T00:09:05.724566Z",
     "shell.execute_reply.started": "2025-10-18T00:09:05.724558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize to lowercase once\n",
    "references = [str(x).lower().strip() for x in pred_df[\"reference\"].fillna(\"\")]\n",
    "predictions = [str(x).lower().strip() for x in pred_df[\"prediction\"].fillna(\"\")]\n",
    "\n",
    "# Simple whitespace tokenizer (already case-insensitive due to lowercase above)\n",
    "def simple_tokenize(text: str):\n",
    "    return text.split()\n",
    "\n",
    "# --- BLEU-1..4 (corpus) ---\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "list_of_refs = [[[t for t in simple_tokenize(ref)]] for ref in references]\n",
    "hyps = [[t for t in simple_tokenize(hyp)] for hyp in predictions]\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "bleu1 = corpus_bleu(list_of_refs, hyps, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=smooth)\n",
    "bleu2 = corpus_bleu(list_of_refs, hyps, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smooth)\n",
    "bleu3 = corpus_bleu(list_of_refs, hyps, weights=(1/3, 1/3, 1/3, 0.0), smoothing_function=smooth)\n",
    "bleu4 = corpus_bleu(list_of_refs, hyps, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "# --- ROUGE-L (average F1) ---\n",
    "from rouge_score import rouge_scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rougeL_f = []\n",
    "for ref, hyp in zip(references, predictions):  # already lowercased\n",
    "    score = rouge.score(ref, hyp)\n",
    "    rougeL_f.append(score['rougeL'].fmeasure)\n",
    "rougeL = float(sum(rougeL_f) / max(1, len(rougeL_f)))\n",
    "\n",
    "# --- CIDEr ---\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "gts = {i: [references[i]] for i in range(len(references))}   # lowercased refs\n",
    "res = {i: [predictions[i]] for i in range(len(predictions))} # lowercased hyps\n",
    "cider_scorer = Cider()\n",
    "cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "\n",
    "# --- BERTScore ---\n",
    "from bert_score import score as bertscore\n",
    "P, R, F1 = bertscore(predictions, references, lang='en', rescale_with_baseline=True)  # lowercased inputs\n",
    "bertscore_precision = float(P.mean().item())\n",
    "bertscore_recall = float(R.mean().item())\n",
    "bertscore_f1 = float(F1.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-18T00:09:05.725447Z",
     "iopub.status.idle": "2025-10-18T00:09:05.725688Z",
     "shell.execute_reply": "2025-10-18T00:09:05.725588Z",
     "shell.execute_reply.started": "2025-10-18T00:09:05.725577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"BLEU-1\": round(bleu1, 4),\n",
    "    \"BLEU-2\": round(bleu2, 4),\n",
    "    \"BLEU-3\": round(bleu3, 4),\n",
    "    \"BLEU-4\": round(bleu4, 4),\n",
    "    \"ROUGE-L\": round(rougeL, 4),\n",
    "    \"CIDEr\": round(float(cider_score), 4),\n",
    "    \"BERTScore\": {\n",
    "        \"Precision\": round(bertscore_precision, 4),\n",
    "        \"Recall\": round(bertscore_recall, 4),\n",
    "        \"F1\": round(bertscore_f1, 4),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "with open(SCORES_JSON, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved predictions to: {PREDICTIONS_CSV}\")\n",
    "print(f\"Saved metrics to: {SCORES_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T07:16:02.723735Z",
     "iopub.status.busy": "2025-10-18T07:16:02.722928Z",
     "iopub.status.idle": "2025-10-18T07:17:28.842338Z",
     "shell.execute_reply": "2025-10-18T07:17:28.841662Z",
     "shell.execute_reply.started": "2025-10-18T07:16:02.723711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute corpus metrics from a JSONL built during inference\n",
    "# Expects each line: {\"image_id\": \"...\", \"pred\": \"...\", \"reference\": \"...\", ...}\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Metrics\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "PREDICTIONS_JSONL = \"predictions.jsonl\"         # input JSONL with pred/reference\n",
    "SCORES_JSON = \"scores_from_jsonl.json\"          # output summary JSON\n",
    "\n",
    "# 1) Load predictions + references from JSONL (latest per image_id wins)\n",
    "pred_map = {}  # image_id -> (pred, reference)\n",
    "with open(PREDICTIONS_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            image_id = str(obj.get(\"image_id\", \"\")).strip()\n",
    "            pred = str(obj.get(\"pred\", \"\"))\n",
    "            ref = str(obj.get(\"reference\", \"\"))\n",
    "            if image_id:\n",
    "                pred_map[image_id] = (pred, ref)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# 2) Build lists and normalize to lowercase\n",
    "image_ids = []\n",
    "predictions_raw, references_raw = [], []\n",
    "for k, (pred, ref) in pred_map.items():\n",
    "    image_ids.append(k)\n",
    "    predictions_raw.append(pred)\n",
    "    references_raw.append(ref)\n",
    "\n",
    "predictions = [p.lower().strip() for p in predictions_raw]\n",
    "references = [r.lower().strip() for r in references_raw]\n",
    "\n",
    "# 3) Simple tokenizer (whitespace)\n",
    "def simple_tokenize(text: str):\n",
    "    return text.split()\n",
    "\n",
    "# 4) BLEU-1..4 (corpus)\n",
    "list_of_refs = [[[t for t in simple_tokenize(r)]] for r in references]\n",
    "hyps = [[t for t in simple_tokenize(h)] for h in predictions]\n",
    "smooth = SmoothingFunction().method4\n",
    "bleu1 = corpus_bleu(list_of_refs, hyps, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=smooth)\n",
    "bleu2 = corpus_bleu(list_of_refs, hyps, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smooth)\n",
    "bleu3 = corpus_bleu(list_of_refs, hyps, weights=(1/3, 1/3, 1/3, 0.0), smoothing_function=smooth)\n",
    "bleu4 = corpus_bleu(list_of_refs, hyps, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "# 5) ROUGE-L (average F1 over pairs)\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rougeL_vals = [rouge.score(r, h)['rougeL'].fmeasure for r, h in zip(references, predictions)]\n",
    "rougeL = float(sum(rougeL_vals) / max(1, len(rougeL_vals)))\n",
    "\n",
    "# 6) CIDEr (corpus)\n",
    "gts = {i: [references[i]] for i in range(len(references))}\n",
    "res = {i: [predictions[i]] for i in range(len(predictions))}\n",
    "cider_scorer = Cider()\n",
    "cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "\n",
    "# 7) BERTScore (mean P/R/F1)\n",
    "P, R, F1 = bertscore(predictions, references, lang='en', rescale_with_baseline=True)\n",
    "bertscore_precision = float(P.mean().item())\n",
    "bertscore_recall = float(R.mean().item())\n",
    "bertscore_f1 = float(F1.mean().item())\n",
    "\n",
    "# 8) Save and print results\n",
    "results = {\n",
    "    \"BLEU-1\": round(bleu1, 4),\n",
    "    \"BLEU-2\": round(bleu2, 4),\n",
    "    \"BLEU-3\": round(bleu3, 4),\n",
    "    \"BLEU-4\": round(bleu4, 4),\n",
    "    \"ROUGE-L\": round(rougeL, 4),\n",
    "    \"CIDEr\": round(float(cider_score), 4),\n",
    "    \"BERTScore\": {\n",
    "        \"Precision\": round(bertscore_precision, 4),\n",
    "        \"Recall\": round(bertscore_recall, 4),\n",
    "        \"F1\": round(bertscore_f1, 4),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(SCORES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:27:06.273395Z",
     "iopub.status.busy": "2025-10-18T08:27:06.272945Z",
     "iopub.status.idle": "2025-10-18T08:27:06.329629Z",
     "shell.execute_reply": "2025-10-18T08:27:06.329038Z",
     "shell.execute_reply.started": "2025-10-18T08:27:06.273375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, re, json, csv\n",
    "import pandas as pd\n",
    "\n",
    "PREDICTIONS_JSONL = \"predictions.jsonl\"\n",
    "CAPTIONS_CSV = \"gt.csv\"\n",
    "\n",
    "def extract_image_id(path: str) -> str:\n",
    "    # Keep only the filename stem (no folders, no extension)\n",
    "    base = os.path.basename(str(path))\n",
    "    stem, _ = os.path.splitext(base)\n",
    "    # Prefer the numeric part if present (e.g., \"Images/10002.jpeg\" -> \"10002\")\n",
    "    m = re.search(r\"\\d+\", stem)\n",
    "    return m.group(0) if m else stem\n",
    "\n",
    "# Keep the last prediction seen per image_id\n",
    "latest = {}\n",
    "with open(PREDICTIONS_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        img_key = obj.get(\"image_id\", \"\")\n",
    "        pred = obj.get(\"reference\", \"\")\n",
    "        if not img_key:\n",
    "            continue\n",
    "        image_id = extract_image_id(img_key)\n",
    "        latest[image_id] = pred\n",
    "\n",
    "# Build DataFrame and save as CSV with proper quoting\n",
    "rows = [{\"image_id\": k, \"caption\": v} for k, v in latest.items()]\n",
    "df = pd.DataFrame(rows).sort_values(\"image_id\")\n",
    "df.to_csv(CAPTIONS_CSV, index=False, quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
    "print(f\"Wrote {len(df)} rows to {CAPTIONS_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8049204,
     "sourceId": 12734145,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
