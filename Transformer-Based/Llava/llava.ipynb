{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-18T11:20:37.435487Z",
     "iopub.status.busy": "2025-10-18T11:20:37.435030Z",
     "iopub.status.idle": "2025-10-18T11:22:13.475650Z",
     "shell.execute_reply": "2025-10-18T11:22:13.474946Z",
     "shell.execute_reply.started": "2025-10-18T11:20:37.435446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.53.2\" \"tokenizers>=0.20.1\" \"huggingface-hub>=0.25.2\" \"accelerate>=0.34.2\" \"peft>=0.13.0\" \"bitsandbytes>=0.45.0\" \"safetensors>=0.4.5\" \"timm>=1.0.9\" \"torchvision>=0.20.0\" \"pillow>=10.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:13.477604Z",
     "iopub.status.busy": "2025-10-18T11:22:13.477049Z",
     "iopub.status.idle": "2025-10-18T11:22:38.193023Z",
     "shell.execute_reply": "2025-10-18T11:22:38.192466Z",
     "shell.execute_reply.started": "2025-10-18T11:22:13.477579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys, math, random, json, gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:38.194355Z",
     "iopub.status.busy": "2025-10-18T11:22:38.193732Z",
     "iopub.status.idle": "2025-10-18T11:22:38.198619Z",
     "shell.execute_reply": "2025-10-18T11:22:38.197910Z",
     "shell.execute_reply.started": "2025-10-18T11:22:38.194335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:38.200293Z",
     "iopub.status.busy": "2025-10-18T11:22:38.200064Z",
     "iopub.status.idle": "2025-10-18T11:22:39.006450Z",
     "shell.execute_reply": "2025-10-18T11:22:39.005748Z",
     "shell.execute_reply.started": "2025-10-18T11:22:38.200277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Base/model/output\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "OUTPUT_DIR = \"/kaggle/working/llava-v16-mistral-7b-caption-lora-final\"\n",
    "\n",
    "# Dataset mount and subpaths\n",
    "DATASET_DIR = \"/kaggle/input/split-10k-dataset/Split Dataset\"\n",
    "CAPTION_CSV = os.path.join(DATASET_DIR, \"train\", \"description_b.csv\")\n",
    "IMAGES_ROOT = os.path.join(DATASET_DIR, \"train\")      # root under which CSV paths live\n",
    "IMAGES_DIR  = os.path.join(IMAGES_ROOT, \"Images\")     # images directory\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:39.007450Z",
     "iopub.status.busy": "2025-10-18T11:22:39.007185Z",
     "iopub.status.idle": "2025-10-18T11:22:44.871012Z",
     "shell.execute_reply": "2025-10-18T11:22:44.870255Z",
     "shell.execute_reply.started": "2025-10-18T11:22:39.007432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load dataset (CSV with file_name, text)\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CAPTION_CSV)\n",
    "\n",
    "# Expect 'file_name' and 'text' from the CSV; rename to the training code's 'image' and 'caption'\n",
    "assert \"file_name\" in df.columns and \"text\" in df.columns, \"CSV must have 'file_name' and 'text' columns.\"\n",
    "df = df.rename(columns={\"file_name\": \"image\", \"text\": \"caption\"})\n",
    "\n",
    "# Robust path resolver for rows like:\n",
    "#  - \"Images/10000.jpeg\"\n",
    "#  - \"train/Images/10000.jpeg\"\n",
    "#  - \"10000.jpeg\"\n",
    "def make_path(p: str) -> str:\n",
    "    p = str(p).strip()\n",
    "    if os.path.isabs(p):\n",
    "        return p\n",
    "    # If CSV already holds 'train/...' join to dataset root\n",
    "    if p.lower().startswith(\"train/\"):\n",
    "        return os.path.join(DATASET_DIR, p)\n",
    "    # If CSV holds 'Images/...' join to train root\n",
    "    if p.lower().startswith(\"images/\") or p.lower().startswith(\"./images/\"):\n",
    "        p2 = p.replace(\"./\", \"\")\n",
    "        return os.path.join(IMAGES_ROOT, p2)\n",
    "    # Otherwise treat as bare filename\n",
    "    return os.path.join(IMAGES_DIR, os.path.basename(p))\n",
    "\n",
    "# Apply resolver\n",
    "df[\"image\"] = df[\"image\"].apply(make_path)\n",
    "\n",
    "# Filter out missing files\n",
    "df = df[df[\"image\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "print(\"Total samples after filtering:\", len(df))\n",
    "\n",
    "# Train/val split (unchanged)\n",
    "val_frac = 0.2\n",
    "val_size = max(0, int(len(df) * val_frac)) if len(df) >= 100 else max(1, int(len(df) * 0.1))\n",
    "\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "df_val = df.iloc[:val_size].reset_index(drop=True)\n",
    "df_train = df.iloc[val_size:].reset_index(drop=True)\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "val_ds = Dataset.from_pandas(df_val)\n",
    "raw = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:44.872271Z",
     "iopub.status.busy": "2025-10-18T11:22:44.871986Z",
     "iopub.status.idle": "2025-10-18T11:22:53.308595Z",
     "shell.execute_reply": "2025-10-18T11:22:53.307828Z",
     "shell.execute_reply.started": "2025-10-18T11:22:44.872247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "\n",
    "# 2) Build chat-templated prompts\n",
    "INSTRUCTION = \"\"\"Describe the image in a paragraph\"\"\"\n",
    "\n",
    "\n",
    "def build_texts(caption: str):\n",
    "    # Prompt only (user + image), used to compute loss mask\n",
    "    template_user = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": INSTRUCTION}]}\n",
    "    ]\n",
    "    prompt_only_text = processor.apply_chat_template(\n",
    "        template_user,\n",
    "        add_generation_prompt=True,   # ends with \"ASSISTANT:\"\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    # Full conversation (include gold caption as assistant)\n",
    "    template_full = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": INSTRUCTION}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": caption}]},\n",
    "    ]\n",
    "    full_text = processor.apply_chat_template(\n",
    "        template_full,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return prompt_only_text, full_text\n",
    "\n",
    "def map_build_texts(ex):\n",
    "    p, f = build_texts(ex[\"caption\"])\n",
    "    return {\"prompt\": p, \"full_text\": f}\n",
    "\n",
    "train_proc = raw[\"train\"].map(map_build_texts, remove_columns=[])\n",
    "val_proc   = raw[\"validation\"].map(map_build_texts, remove_columns=[])\n",
    "\n",
    "# 3) Collator: tokenize text + images on the fly and build labels with prompt masking\n",
    "pad_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:53.309571Z",
     "iopub.status.busy": "2025-10-18T11:22:53.309291Z",
     "iopub.status.idle": "2025-10-18T11:22:53.315487Z",
     "shell.execute_reply": "2025-10-18T11:22:53.314817Z",
     "shell.execute_reply.started": "2025-10-18T11:22:53.309544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Load images\n",
    "    images = [Image.open(item[\"image\"]).convert(\"RGB\") for item in batch]\n",
    "    # Strings\n",
    "    full_texts   = [item[\"full_text\"] for item in batch]\n",
    "    prompt_texts = [item[\"prompt\"] for item in batch]\n",
    "\n",
    "    # Tokenize full inputs (text + images)\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=full_texts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute prompt lengths per sample (no padding) to create loss mask\n",
    "    prompt_lens = [\n",
    "        len(processor.tokenizer(p, add_special_tokens=False).input_ids)\n",
    "        for p in prompt_texts\n",
    "    ]\n",
    "\n",
    "    # Build labels: mask everything up to prompt length; keep only assistant caption tokens\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[:] = -100  # start fully masked\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        ids = inputs[\"input_ids\"][i]\n",
    "        nonpad = ids != pad_id\n",
    "        total_nonpad = int(nonpad.sum().item())\n",
    "        assist_len = total_nonpad - prompt_lens[i]\n",
    "        if assist_len > 0:\n",
    "            target_tokens = ids[nonpad][-assist_len:]\n",
    "            labels[i, -assist_len:] = target_tokens\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:22:53.316492Z",
     "iopub.status.busy": "2025-10-18T11:22:53.316263Z",
     "iopub.status.idle": "2025-10-18T11:25:31.686722Z",
     "shell.execute_reply": "2025-10-18T11:25:31.685951Z",
     "shell.execute_reply.started": "2025-10-18T11:22:53.316469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:25:31.687725Z",
     "iopub.status.busy": "2025-10-18T11:25:31.687526Z",
     "iopub.status.idle": "2025-10-18T11:25:32.582959Z",
     "shell.execute_reply": "2025-10-18T11:25:32.582410Z",
     "shell.execute_reply.started": "2025-10-18T11:25:31.687709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# Prepare for k-bit training and wrap with LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 5) Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=not bf16,\n",
    "    bf16=bf16,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # <-- important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T11:25:32.585062Z",
     "iopub.status.busy": "2025-10-18T11:25:32.584607Z",
     "iopub.status.idle": "2025-10-18T18:54:50.175027Z",
     "shell.execute_reply": "2025-10-18T18:54:50.174328Z",
     "shell.execute_reply.started": "2025-10-18T11:25:32.585045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_proc,\n",
    "    eval_dataset=val_proc,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 7) Save LoRA adapter and processor\n",
    "trainer.save_model(OUTPUT_DIR)  # saves PEFT adapter\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved LoRA adapter + processor to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q sacrebleu rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pip installs (run once per session if needed)\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# Assumes these exist from training\n",
    "# MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "# OUTPUT_DIR = \"/kaggle/working/llava-v16-mistral-7b-caption-lora\"  # change to your adapter dir\n",
    "# df_val has columns: image (absolute path), caption (reference)\n",
    "# INSTRUCTION = \"... your biology-focused instruction ...\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# Load base + LoRA adapter\n",
    "base = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch_dtype, device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, OUTPUT_DIR).eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Helper to build the chat prompt per sample\n",
    "def build_prompt(instruction: str):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}]}]\n",
    "    return processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# Batched generation\n",
    "def generate_captions(rows, batch_size=4, max_new_tokens=200, temperature=0.2, top_p=0.9):\n",
    "    preds = []\n",
    "    refs = []\n",
    "    for i in tqdm(range(0, len(rows), batch_size)):\n",
    "        batch = rows[i:i+batch_size]\n",
    "        images = [Image.open(x[\"image\"]).convert(\"RGB\") for x in batch]\n",
    "        prompts = [build_prompt(INSTRUCTION) for _ in batch]\n",
    "        inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=temperature > 0\n",
    "            )\n",
    "        texts = processor.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Extract only the assistant response after \"ASSISTANT:\"\n",
    "        # If not present, fallback to full decoded text\n",
    "        for t, row in zip(texts, batch):\n",
    "            parts = t.split(\"ASSISTANT:\")\n",
    "            caption = parts[-1].strip() if len(parts) >= 2 else t.strip()\n",
    "            preds.append(caption)\n",
    "            refs.append(row[\"caption\"])\n",
    "    return preds, refs\n",
    "\n",
    "# Prepare rows from df_val\n",
    "val_rows = df_val[[\"image\", \"caption\"]].to_dict(orient=\"records\")\n",
    "preds, refs = generate_captions(val_rows, batch_size=4)\n",
    "print(f\"Generated {len(preds)} captions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_ids = [os.path.basename(r[\"image\"]) for r in val_rows]\n",
    "\n",
    "# Create DataFrames\n",
    "preds_df = pd.DataFrame({\"Image_id\": image_ids, \"caption\": preds})\n",
    "refs_df  = pd.DataFrame({\"Image_id\": image_ids, \"caption\": refs})\n",
    "both_df  = pd.DataFrame({\"Image_id\": image_ids, \"pred_caption\": preds, \"ref_caption\": refs})\n",
    "\n",
    "# Save under /kaggle/working so they appear in the Output panel and are easy to download\n",
    "preds_csv = \"/kaggle/working/val_preds.csv\"\n",
    "refs_csv  = \"/kaggle/working/val_refs.csv\"\n",
    "both_csv  = \"/kaggle/working/val_preds_refs.csv\"\n",
    "\n",
    "preds_df.to_csv(preds_csv, index=False, encoding=\"utf-8\")\n",
    "refs_df.to_csv(refs_csv, index=False, encoding=\"utf-8\")\n",
    "both_df.to_csv(both_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(preds_csv)\n",
    "print(refs_csv)\n",
    "print(both_csv)\n",
    "\n",
    "# Provide clickable links in the notebook\n",
    "display(FileLink(os.path.basename(preds_csv)))\n",
    "display(FileLink(os.path.basename(refs_csv)))\n",
    "display(FileLink(os.path.basename(both_csv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Metrics: SacreBLEU (BLEU-4), ROUGE-L, and optional BERTScore\n",
    "\n",
    "# BLEU via SacreBLEU\n",
    "import sacrebleu\n",
    "bleu = sacrebleu.corpus_bleu(preds, [refs])  # single reference per sample\n",
    "print(f\"BLEU (SacreBLEU): {bleu.score:.2f}\")\n",
    "\n",
    "# ROUGE-L via rouge-score\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "r_f1s = [scorer.score(r, p)[\"rougeL\"].fmeasure for p, r in zip(preds, refs)]\n",
    "print(f\"ROUGE-L F1: {100 * (sum(r_f1s) / len(r_f1s)):.2f}\")\n",
    "\n",
    "# Optional: BERTScore (semantic)\n",
    "use_bertscore = True\n",
    "if use_bertscore:\n",
    "    from bert_score import score as bertscore\n",
    "    P, R, F1 = bertscore(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "    print(f\"BERTScore P/R/F1: {100*P.mean().item():.2f} / {100*R.mean().item():.2f} / {100*F1.mean().item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# In a Kaggle Notebook cell\n",
    "%cd /kaggle/working\n",
    "!zip -r working_dir.zip . -x \"working_dir.zip\"\n",
    "\n",
    "\n",
    "FileLink('working_dir.zip')  # click to download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q pycocoevalcap\n",
    "# Fallback if needed:\n",
    "# pip install -q \"git+https://github.com/salaniz/pycocoevalcap.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "\n",
    "# Inputs from inference\n",
    "# preds: list[str]  # model-generated captions, length N\n",
    "# refs:  list[str]  # reference captions (one per sample), length N\n",
    "\n",
    "# Build COCO-style dicts: id -> [captions]\n",
    "res = {str(i): [preds[i]] for i in range(len(preds))}\n",
    "gts = {str(i): [refs[i]]  for i in range(len(refs))}\n",
    "\n",
    "bleu = Bleu(n=4)\n",
    "scores, _ = bleu.compute_score(gts, res)  # scores is a list of 4 floats (BLEU-1..4)\n",
    "bleu1, bleu2, bleu3, bleu4 = [s for s in scores]\n",
    "print(f\"BLEU-1: {bleu1:.2f}\")\n",
    "print(f\"BLEU-2: {bleu2:.2f}\")\n",
    "print(f\"BLEU-3: {bleu3:.2f}\")\n",
    "print(f\"BLEU-4: {bleu4:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8049204,
     "sourceId": 12734145,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
