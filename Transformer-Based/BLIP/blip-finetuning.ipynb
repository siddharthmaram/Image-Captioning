{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13065678,"sourceType":"datasetVersion","datasetId":8274338}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm.notebook import tqdm\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom transformers import AutoProcessor, BlipForConditionalGeneration\nfrom peft import LoraConfig, get_peft_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageCaptionDataset(Dataset):\n    def __init__(self, csv_file, image_folder, processor):\n        self.data = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = os.path.join(self.image_folder, row[\"file_name\"])\n        \n        with Image.open(img_path) as im:\n            if im.mode == \"P\":\n                im = im.convert(\"RGBA\")\n            image = im.convert(\"RGB\")\n        \n        caption = row[\"text\"]\n        \n        # ✅ Process image and text together (like your working script)\n        encoding = self.processor(\n            images=image,\n            text=caption,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        # Remove batch dimension\n        encoding = {k: v.squeeze() for k, v in encoding.items()}\n        return encoding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============ Training Script ============ #\nfrom transformers import get_cosine_schedule_with_warmup\n\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/biology-dataset/Biology-Dataset/train/metadata.csv\"\n    train_folder = \"/kaggle/input/biology-dataset/Biology-Dataset/train/\"\n\n    val_csv = \"/kaggle/input/biology-dataset/Biology-Dataset/val/metadata.csv\"\n    val_folder = \"/kaggle/input/biology-dataset/Biology-Dataset/val/\"\n\n    # Device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    model = BlipForConditionalGeneration.from_pretrained(\n        \"/kaggle/working/best_model_epoch\",\n        device_map=\"auto\"\n    )\n\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # 2. Unfreeze last 2 text decoder layers\n    for i in [-1, -4]:\n        for param in model.text_decoder.bert.encoder.layer[i].parameters():\n            param.requires_grad = True\n    \n    # 3. Unfreeze LM head (for vocab adaptation)\n    for param in model.text_decoder.cls.predictions.parameters():\n        param.requires_grad = True\n\n    for i in [-1, -4]:\n        layer = model.text_decoder.bert.encoder.layer[i]\n        if hasattr(layer, \"crossattention\"):\n            for param in layer.crossattention.parameters():\n                param.requires_grad = True\n    # Datasets\n    train_dataset = ImageCaptionDataset(train_csv, train_folder, processor)\n    val_dataset = ImageCaptionDataset(val_csv, val_folder, processor)\n\n    # Dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=8,\n        shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=8,\n        shuffle=False,\n    )\n    \n    num_epochs = 10\n    total_steps = len(train_loader) * num_epochs\n\n    # Optimizer\n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    # optimizer = AdamW(model.parameters(), lr=1e-5)\n    optimizer = AdamW(trainable_params, lr=1e-5)\n    print(f\"Trainable params: {sum(p.numel() for p in trainable_params)}\")\n    # optimizer = AdamW(\n    #     trainable_params,\n    #     lr=1e-5,\n    #     weight_decay=0.01\n    # )\n    # scheduler = get_cosine_schedule_with_warmup(\n    #     optimizer,\n    #     num_warmup_steps=500,\n    #     num_training_steps=total_steps\n    # )\n\n    # Training loop\n    # best_val_loss = float(\"inf\")\n    best_val_loss = 0.7055\n    model.train()\n\n    for epoch in range(37, num_epochs+37):\n        print(f\"\\nEpoch {epoch}\")\n\n        # Training\n        total_loss = 0.0\n        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n        for step, batch in enumerate(train_bar):\n            input_ids = batch.pop(\"input_ids\").to(device)\n            pixel_values = batch.pop(\"pixel_values\").to(device)\n            attention_mask = batch.pop(\"attention_mask\").to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask, \n                labels=input_ids\n            )\n\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += loss.item()\n            train_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch} | Avg Train Loss: {avg_train_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n        with torch.no_grad():\n            for batch in val_bar:\n                input_ids = batch.pop(\"input_ids\").to(device)\n                pixel_values = batch.pop(\"pixel_values\").to(device)\n                attention_mask = batch.pop(\"attention_mask\").to(device)\n    \n                outputs = model(\n                    input_ids=input_ids,\n                    pixel_values=pixel_values,\n                    attention_mask=attention_mask,  # ✅ Pass attention mask\n                    labels=input_ids\n                )\n\n                val_loss += outputs.loss.item()\n                val_bar.set_postfix({\"loss\": f\"{outputs.loss.item():.4f}\"})\n\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Epoch {epoch} | Avg Val Loss: {avg_val_loss:.4f}\")\n\n        # Save best checkpoint\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            model.save_pretrained(f\"best_model_epoch\")\n            print(f\"best_model_epoch saved\")\n\n        model.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install pycocoevalcap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.rouge.rouge import Rouge\n\nimport json\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, csv_file, image_folder, processor):\n        self.data = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = os.path.join(self.image_folder, row[\"file_name\"])\n        with Image.open(img_path) as im:\n            if im.mode == \"P\":\n                im = im.convert(\"RGBA\")\n            image = im.convert(\"RGB\")\n        caption = row[\"text\"]\n        return {\"image\": image, \"caption\": caption, \"id\": idx}\n\ndef collate_fn(batch, processor):\n    images = [item[\"image\"] for item in batch]\n    captions = [item[\"caption\"] for item in batch]\n    ids = [item[\"id\"] for item in batch]\n    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n    return pixel_values, captions, ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_csv = \"/kaggle/input/biology-dataset/Biology-Dataset/test/metadata.csv\"\ntest_folder = \"/kaggle/input/biology-dataset/Biology-Dataset/test\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load processor & base model\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\n    \"/kaggle/working/best_model_epoch\",\n    device_map=\"auto\"\n)\n\nmodel.eval()\n\n# Dataset + DataLoader\ntest_dataset = ImageCaptionDataset(test_csv, test_folder, processor)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=4,           # CHANGED: Smaller for more stable generation\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, processor)\n)\n\n# Collect predictions and references\ngts = {}   # ground truth {id: [captions]}\nres = {}   # predictions {id: [caption]}\n\nprint(\"Generating captions...\")\nwith torch.inference_mode():\n    for pixel_values, captions, ids in tqdm(test_loader, desc=\"Evaluating\"):\n        pixel_values = pixel_values.to(device)\n\n        # OPTIMIZED: Much faster generation settings\n        generated_ids = model.generate(\n            pixel_values=pixel_values,\n            max_length=512,              # CHANGED: 64 instead of 1024 (16x faster!)\n            num_beams=3,                # CHANGED: 3 beams for better quality\n            early_stopping=True,       # ADDED: Stop when EOS token found\n            repetition_penalty=1.2,    # ADDED: Prevent repetition\n            no_repeat_ngram_size=3,    # ADDED: Avoid repetitive phrases\n        )\n        \n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n\n        for i, idx in enumerate(ids):\n            gts[idx] = [captions[i].lower()]                    # ground truth\n            res[idx] = [generated_text[i].strip().lower()]      # model prediction (stripped)\n        \n        # ADDED: Clear GPU cache periodically\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nprint(f\"Generated {len(res)} captions\")\n\n# Save predictions to file\nwith open(\"predictions_freezed2.json\", \"w\") as f:\n    json.dump(res, f, indent=2)\n\n# ============ Metrics ============ #\nprint(\"\\n==== Evaluation Results ====\")\n\n# BLEU-1 to BLEU-4\ntry:\n    bleu_scorer = Bleu(4)\n    bleu_score, _ = bleu_scorer.compute_score(gts, res)\n    print(f\"BLEU-1: {bleu_score[0]:.4f}\")\n    print(f\"BLEU-2: {bleu_score[1]:.4f}\")\n    print(f\"BLEU-3: {bleu_score[2]:.4f}\")\n    print(f\"BLEU-4: {bleu_score[3]:.4f}\")\nexcept Exception as e:\n    print(f\"Error computing BLEU: {e}\")\n\n# ROUGE-L\ntry:\n    rouge_scorer = Rouge()\n    rouge_score, _ = rouge_scorer.compute_score(gts, res)\n    print(f\"ROUGE-L: {rouge_score:.4f}\")\nexcept Exception as e:\n    print(f\"Error computing ROUGE: {e}\")\n\n# CIDEr\ntry:\n    cider_scorer = Cider()\n    cider_score, _ = cider_scorer.compute_score(gts, res)\n    print(f\"CIDEr: {cider_score:.4f}\")\nexcept Exception as e:\n    print(f\"Error computing CIDEr: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}