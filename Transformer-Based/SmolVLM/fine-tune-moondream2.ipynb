{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13316276,"sourceType":"datasetVersion","datasetId":8441638}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install \"einops==0.7.0\" \"bitsandbytes==0.45.2\" \"accelerate>=0.20.1\" \"ipywidgets>=8.1\" \"jupyterlab-widgets>=3\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ipywidgets, sys\nimport jupyterlab_widgets\nprint(\"ipywidgets:\", ipywidgets.__version__)         # Expect 8.x\nprint(\"jupyterlab-widgets:\", jupyterlab_widgets.__version__)  # Expect 3.x on Lab 4\nprint(sys.version)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader\nfrom bitsandbytes.optim import Adam8bit\nimport math\nfrom einops import rearrange\nfrom tqdm import tqdm\nfrom huggingface_hub import notebook_login\nfrom PIL import Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logging to Hugging Face","metadata":{}},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing Datasets","metadata":{}},{"cell_type":"code","source":"class DentalDataset(Dataset):\n    def __init__(self, split='train'):\n        self.data = load_dataset(\"imagefolder\", data_dir=\"/kaggle/input/biology-dataset/Biology-Dataset\", split=split)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        return {\n            \"image\": sample[\"image\"], # Should be a PIL image\n            \"qa\": [\n                {\n                    \"question\": \"Describe this image.\",\n                    \"answer\": sample[\"text\"],\n                }\n            ]\n        }\n\ndatasets = {\n    \"train\": DentalDataset(\"train\"),\n    \"test\": DentalDataset(\"test\"),\n    \"validation\": DentalDataset(\"validation\")\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load base model","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\"\nDTYPE = torch.float32 if DEVICE == \"cpu\" else torch.float16 # CPU doesn't support float16\nMD_REVISION = \"2024-05-20\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\nmoondream = AutoModelForCausalLM.from_pretrained(\n    \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n    torch_dtype=DTYPE, device_map={\"\": DEVICE}\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate model before fine-tuning","metadata":{}},{"cell_type":"code","source":"sample = datasets['train'][0]\ndisplay(sample['image'])\n\nfor qa in sample['qa']:\n    print('Question:\\n', qa['question'])\n    print('Ground Truth:\\n', qa['answer'])\n    print('Moondream:\\n', moondream.answer_question(\n        moondream.encode_image(sample['image']),\n        qa['question'],\n        tokenizer=tokenizer,\n    ))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting up hyperparameters for fine-tuning","metadata":{}},{"cell_type":"code","source":"EPOCHS = 1\nBATCH_SIZE = 4\nGRAD_ACCUM_STEPS = 2\nLR = 1e-5\nUSE_WANDB = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ANSWER_EOS = \"<|endoftext|>\"\n\n# Number of tokens used to represent each image.\nIMG_TOKENS = 729\n\ndef collate_fn(batch):\n    images = [sample['image'] for sample in batch]\n    images = [img.convert(\"RGB\") if hasattr(img, \"mode\") and img.mode != \"RGB\" else img for img in images]\n    images = [moondream.vision_encoder.preprocess(image) for image in images]\n\n    labels_acc = []\n    tokens_acc = []\n\n    for sample in batch:\n        toks = [tokenizer.bos_token_id]\n        labs = [-100] * (IMG_TOKENS + 1)\n\n        for qa in sample['qa']:\n            q_t = tokenizer(\n                f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\",\n                add_special_tokens=False\n            ).input_ids\n            toks.extend(q_t)\n            labs.extend([-100] * len(q_t))\n\n            a_t = tokenizer(\n                f\" {qa['answer']}{ANSWER_EOS}\",\n                add_special_tokens=False\n            ).input_ids\n            toks.extend(a_t)\n            labs.extend(a_t)\n\n        tokens_acc.append(toks)\n        labels_acc.append(labs)\n\n    max_len = -1\n    for labels in labels_acc:\n        max_len = max(max_len, len(labels))\n\n    attn_mask_acc = []\n\n    for i in range(len(batch)):\n        len_i = len(labels_acc[i])\n        pad_i = max_len - len_i\n\n        labels_acc[i].extend([-100] * pad_i)\n        tokens_acc[i].extend([tokenizer.eos_token_id] * pad_i)\n        attn_mask_acc.append([1] * len_i + [0] * pad_i)\n\n    return (\n        images,\n        torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n        torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n        torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n    )\n\ndef compute_loss(batch):\n    images, tokens, labels, attn_mask = batch\n\n    tokens = tokens.to(DEVICE)\n    labels = labels.to(DEVICE)\n    attn_mask = attn_mask.to(DEVICE)\n\n    with torch.no_grad():\n        img_embs = moondream.vision_encoder(images)\n\n    tok_embs = moondream.text_model.get_input_embeddings()(tokens)\n    inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n\n    outputs = moondream.text_model(\n        inputs_embeds=inputs_embeds,\n        labels=labels,\n        attention_mask=attn_mask,\n    )\n\n    return outputs.loss\n\ndef lr_schedule(step, max_steps):\n    x = step / max_steps\n    if x < 0.1:\n        return 0.1 * LR + 0.9 * LR * x / 0.1\n    else:\n        return 0.1 * LR + 0.9 * LR * (1 + math.cos(math.pi * (x - 0.1))) / 2\n\ndef evaluate(model, dataloader):\n    model.eval()\n    total_loss = 0\n    count = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            loss = compute_loss(batch)\n            total_loss += loss.item()\n            count += 1\n\n    model.train()\n    return total_loss / count\n\ndataloaders = {\n    \"train\": DataLoader(\n        datasets[\"train\"],\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        collate_fn=collate_fn,\n    ),\n    \"validation\": DataLoader(\n        datasets[\"validation\"],\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n}\n\nmoondream.text_model.train()\nmoondream.text_model.transformer.gradient_checkpointing_enable()\n\ntotal_steps = EPOCHS * len(dataloaders[\"train\"]) // GRAD_ACCUM_STEPS\noptimizer = Adam8bit(\n    [\n        {\"params\": moondream.text_model.parameters()},\n    ],\n    lr=LR * 0.1,\n    betas=(0.9, 0.95),\n    eps=1e-6\n)\n\nif USE_WANDB:\n    import wandb\n    wandb.init(\n        project=\"moondream-ft\",\n        config={\n            \"EPOCHS\": EPOCHS,\n            \"BATCH_SIZE\": BATCH_SIZE,\n            \"GRAD_ACCUM_STEPS\": GRAD_ACCUM_STEPS,\n            \"LR\": LR,\n        }\n    )\n\ni = 0\nfor epoch in range(EPOCHS):\n    for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n        i += 1\n\n        loss = compute_loss(batch)\n        loss.backward()\n\n        if i % GRAD_ACCUM_STEPS == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n            lr = lr_schedule(i / GRAD_ACCUM_STEPS, total_steps)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        if USE_WANDB:\n            wandb.log({\n                \"loss/train\": loss.item(),\n                \"lr\": optimizer.param_groups[0]['lr']\n            })\n        if i % 250 == 0:\n            val_loss = evaluate(moondream.text_model, dataloaders[\"validation\"])\n            print(f\"Validation Loss (step {i+1}): {val_loss:.4f}\")\n\n            if USE_WANDB:\n                wandb.log({\"loss/val\": val_loss})\n\nif USE_WANDB:\n    wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the fine-tuned model","metadata":{}},{"cell_type":"code","source":"moondream.save_pretrained(\"checkpoints/moondream-ft\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Push to Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"moondream.push_to_hub(\"moondream-ft\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reload the fine-tuned model for evaluation\n","metadata":{}},{"cell_type":"code","source":"# moondream = AutoModelForCausalLM.from_pretrained(\"./checkpoints/moondream-ft\", trust_remote_code=True)\n# moondream.eval()\nfrom safetensors.torch import load_file\n\nDEVICE = \"cuda\"\nDTYPE = torch.float32 if DEVICE == \"cpu\" else torch.float16 # CPU doesn't support float16\nMD_REVISION = \"2024-05-20\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n    torch_dtype=DTYPE, device_map={\"\": DEVICE}\n)\n\nstate_dict = load_file(\"checkpoints/moondream-ft/model.safetensors\", device=\"cpu\")\n\n# 3. Apply weights to the model\nmissing, unexpected = base_model.load_state_dict(state_dict, strict=False)\nprint(\"✅ Loaded fine-tuned weights.\")\nprint(\"Missing keys:\", missing)\nprint(\"Unexpected keys:\", unexpected)\n\nmoondream2 = base_model.eval().to(DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation and Inference\n","metadata":{}},{"cell_type":"code","source":"# for i, sample in enumerate(datasets['test']):\n#     md_answer = moondream.answer_question(\n#         moondream.encode_image(sample['image']),\n#         sample['qa'][0]['question'],\n#         tokenizer=tokenizer,\n#         num_beams=4,\n#         no_repeat_ngram_size=5,\n#         early_stopping=True\n#     )\n\n#     if i < 8:\n#         display(sample['image'])\n#         print('Question:', sample['qa'][0]['question'])\n#         print('Ground Truth:', sample['qa'][0]['answer'])\n#         print('Moondream:', md_answer)\n#     else:\n#         break\n\nimport json\n\nresults = []\n\nfor i, sample in tqdm(enumerate(datasets['test'])):\n    img = sample[\"image\"]\n    img = img.convert(\"RGB\") if hasattr(img, \"mode\") and img.mode != \"RGB\" else img\n    md_answer = moondream2.answer_question(\n        moondream2.encode_image(img),\n        sample['qa'][0]['question'],\n        tokenizer=tokenizer,\n        num_beams=4,\n        no_repeat_ngram_size=5,\n        early_stopping=True\n    )\n\n    results.append({\n        \"id\": i,\n        \"question\": sample['qa'][0]['question'],\n        \"ground_truth\": sample['qa'][0]['answer'],\n        \"prediction\": md_answer\n    })\n\n    if i < 8:\n        display(sample['image'])\n        print('Question:', sample['qa'][0]['question'])\n        print('Ground Truth:', sample['qa'][0]['answer'])\n        print('Moondream:', md_answer)\n    # optional limit\n    # else:\n    #     break\n\n# save to disk\nwith open(\"predictions.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(\"✅ Saved predictions to predictions.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}