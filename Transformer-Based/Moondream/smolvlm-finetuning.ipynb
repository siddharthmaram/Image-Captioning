{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13137286,"sourceType":"datasetVersion","datasetId":8322897}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate -q\n!pip install bitsandbytes -q\n!pip install peft -q\n!pip install trl==0.11.4 -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_dataset = load_dataset(\"imagefolder\", data_dir=\"/kaggle/input/biology-dataset-modified/Biology-Dataset\", split=\"train\")\neval_dataset = load_dataset(\"imagefolder\", data_dir=\"/kaggle/input/biology-dataset-modified/Biology-Dataset\", split=\"validation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, torch, gc\nfrom transformers import AutoModelForVision2Seq, AutoProcessor, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\n\n# 1) Set allocator tuning BEFORE torch/model loads\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"  # fragmentation fix\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # avoid wandb stalls\n\n# 2) Clear memory\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    torch.cuda.set_device(0)\n\n# 3) Load model\nmodel_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\nmodel = AutoModelForVision2Seq.from_pretrained(\n    model_id,\n    device_map={\"\": 0},\n    torch_dtype=torch.float16,  # fp16 for T4; use bfloat16 on A100/L4\n    _attn_implementation=\"eager\",\n    trust_remote_code=True,\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# 4) Save memory: no KV cache while training\nmodel.config.use_cache = False\nmodel.train()   \n\n# 5) PEFT LoRA (keep small)\npeft_config = LoraConfig(\n    r=8, lora_alpha=16, lora_dropout=0.1,\n    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n    use_dora=False,  # simpler, less overhead\n)\nmodel = get_peft_model(model, peft_config)\nmodel.enable_input_require_grads()\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./smolvlm-256m-caption-model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    logging_steps=25,\n    save_steps=500,\n    save_total_limit=1,\n    fp16=True,\n    gradient_checkpointing=True,\n    remove_unused_columns=False,\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n    eval_strategy=\"no\",     # or \"epoch\"\n\n    report_to=\"none\",\n    optim=\"adamw_8bit\",\n)\n\ndef smolvlm_collate_fn(examples):\n    texts, images = [], []\n    for ex in examples:\n        # build chat text\n        messages = [\n            {\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":\"Describe this image.\"}]},\n            {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":str(ex[\"text\"])}]},\n        ]\n        txt = processor.apply_chat_template(messages, tokenize=False)\n        texts.append(txt)\n        img = ex[\"image\"]\n        if hasattr(img, \"convert\") and getattr(img, \"mode\", \"\") != \"RGB\":\n            img = img.convert(\"RGB\")\n        images.append([img])  # processor expects list-of-images per sample\n\n    batch = processor(\n        text=texts,\n        images=images,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=2048\n    )\n    # Trainer expects labels if training LM\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,  # or set eval off if not using\n    data_collator=smolvlm_collate_fn,\n    tokenizer=processor.tokenizer,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbase_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"        # base model\nadapter_dir = \"/kaggle/working/smolvlm-256m-caption-model/\"  # your trainer.save_model() path\n\nprocessor = AutoProcessor.from_pretrained(base_id)\n\n# load base\nmodel = AutoModelForVision2Seq.from_pretrained(\n    base_id, trust_remote_code=True, _attn_implementation=\"eager\"\n).to(device)  # in-place API below [web:451][web:435]\n\n# attach adapter IN PLACE (do NOT assign)\nmodel.load_adapter(adapter_dir, adapter_name=\"default\")  # returns None by design [web:451]\n\n# select adapter if multiple exist\nif hasattr(model, \"set_adapter\"):\n    model.set_adapter(\"default\")  # optional but recommended [web:451]\n\nmodel.eval()\n\n# Build a prompt and run inference\nimg_path = \"/kaggle/input/biology-dataset-modified/Biology-Dataset/test\"\nimg = os.path.join(img_path, \"1.png\")\nimage = Image.open(img).convert(\"RGB\")\nmessages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}]\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n\ninputs = processor(text=prompt, images=[[image]], return_tensors=\"pt\").to(device)\nwith torch.inference_mode():\n    out = model.generate(**inputs, max_new_tokens=200, do_sample=False)\ncaption = processor.batch_decode(out[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\nprint(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install evaluate sacrebleu rouge-score bert-score pycocoevalcap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport evaluate\nfrom statistics import mean\n\n# Load metrics once\nbleu_metric = evaluate.load(\"bleu\")              # sacrebleu-backed BLEU [0..1]\nrouge_metric = evaluate.load(\"rouge\")            # returns rouge1/2/L/Lsum\nbertscore_metric = evaluate.load(\"bertscore\")    # returns precision/recall/f1 lists\n\n# CIDEr from pycocoevalcap (expects dict format)\nfrom pycocoevalcap.cider.cider import Cider\n\ndef compute_bleu(preds, refs):\n    # refs: list of list[str] (multi-ref) or list[str]; convert to list[list]\n    norm_refs = [[r] if isinstance(r, str) else r for r in refs]\n    out = bleu_metric.compute(predictions=preds, references=norm_refs)\n    return {\"bleu\": 100.0 * float(out[\"bleu\"])}  # scale to [0,100]\n\ndef compute_rougeL(preds, refs):\n    # For multiple refs, use the first or the best; here we take first if list\n    single_refs = [r[0] if isinstance(r, list) else r for r in refs]\n    out = rouge_metric.compute(predictions=preds, references=single_refs)\n    return {\"rougeL_f\": 100.0 * float(out[\"rougeL\"])}  # ROUGE-L F1 in %\n\ndef compute_bertscore(preds, refs, model_type=\"roberta-large\", lang=\"en\"):\n    single_refs = [r[0] if isinstance(r, list) else r for r in refs]\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    out = bertscore_metric.compute(\n        predictions=preds,\n        references=single_refs,\n        model_type=model_type,\n        lang=lang,\n        device=device,\n        rescale_with_baseline=True    # common practice for comparability\n    )\n    # Return average F1 (plus P/R if desired)\n    return {\n        \"bertscore_f1\": 100.0 * float(mean(out[\"f1\"])),\n        \"bertscore_p\":  100.0 * float(mean(out[\"precision\"])),\n        \"bertscore_r\":  100.0 * float(mean(out[\"recall\"])),\n        \"model\": model_type,\n    }\n\ndef compute_cider(preds, refs):\n    # Convert to COCO dicts: keys must be strings; values are lists of strings\n    gts = {str(i): (refs[i] if isinstance(refs[i], list) else [refs[i]]) for i in range(len(preds))}\n    res = {str(i): [preds[i]] for i in range(len(preds))}\n    score, _ = Cider().compute_score(gts, res)\n    return {\"cider\": float(score)}  # typically already on a 0–10ish scale (often reported ×10)\n\ndef compute_all(preds, refs, model_type=\"roberta-large\", lang=\"en\"):\n    metrics = {}\n    metrics.update(compute_bleu(preds, refs))\n    metrics.update(compute_rougeL(preds, refs))\n    metrics.update(compute_bertscore(preds, refs, model_type=model_type, lang=lang))\n    metrics.update(compute_cider(preds, refs))\n    return metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nimport os\nimport csv\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbase_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"        # base model\nadapter_dir = \"/kaggle/working/smolvlm-256m-caption-model/\"  # your trainer.save_model() path\n\nprocessor = AutoProcessor.from_pretrained(base_id)\n\n# load base\nmodel = AutoModelForVision2Seq.from_pretrained(\n    base_id, trust_remote_code=True, _attn_implementation=\"eager\"\n).to(device)  # in-place API below [web:451][web:435]\n\n# attach adapter IN PLACE (do NOT assign)\nmodel.load_adapter(adapter_dir, adapter_name=\"default\")  # returns None by design [web:451]\n\n# select adapter if multiple exist\nif hasattr(model, \"set_adapter\"):\n    model.set_adapter(\"default\")  # optional but recommended [web:451]\n\n\nTEST_PATH =  \"/kaggle/input/biology-dataset-modified/Biology-Dataset/test\"\n\n\ndef inference(model):\n    refs = []\n    preds = []\n    model.eval()\n    test_csv = os.path.join(TEST_PATH, \"metadata.csv\")\n    with open(test_csv, \"r\") as f:\n        reader = csv.reader(f)\n        _ = next(reader)\n        for img, caption in tqdm(reader):\n            refs.append(caption)\n            img_path = os.path.join(TEST_PATH, img)\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}]\n            prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n            \n            inputs = processor(text=prompt, images=[[image]], return_tensors=\"pt\").to(device)\n            \n            with torch.inference_mode():\n                out = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n            gen_caption = processor.batch_decode(out[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n            preds.append(gen_caption)\n    return refs, preds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"refs, preds = inference(model)\nscores = compute_all(preds, refs, model_type=\"roberta-large\", lang=\"en\")\nprint(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\n\ntest_csv = os.path.join(TEST_PATH, \"metadata.csv\")\npredictions = []\nwith open(test_csv, \"r\") as f:\n    reader = csv.reader(f)\n    _ = next(reader)\n    for ind, (img, caption) in enumerate(reader):\n        predictions.append([img, preds[ind]])\n        \nwith open(\"preds.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"image\", \"generated_caption\"])\n    writer.writerows(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_files = ['391.jpeg', '1.png', '137.png', '235.png', '378.png', '580.png', \n            '715.png', '817.png', '879.png', '2152.png', '2403.jpeg', \n            '2807.jpeg', '4075.jpeg', '4099.png', '4444.png', '4531.png', \n            '4672.jpeg', '6143.jpeg', '6308.jpeg', '6896.jpeg', \n            '7068.png', '7539.png', '8032.png', '8433.jpeg']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nimport os\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbase_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"        # base model\nadapter_dir = \"/kaggle/working/smolvlm-256m-caption-model/\"  # your trainer.save_model() path\n\nprocessor = AutoProcessor.from_pretrained(base_id)\n\n# load base\nmodel = AutoModelForVision2Seq.from_pretrained(\n    base_id, trust_remote_code=True, _attn_implementation=\"eager\"\n).to(device)  # in-place API below [web:451][web:435]\n\n# attach adapter IN PLACE (do NOT assign)\nmodel.load_adapter(adapter_dir, adapter_name=\"default\")  # returns None by design [web:451]\n\n# select adapter if multiple exist\nif hasattr(model, \"set_adapter\"):\n    model.set_adapter(\"default\")  # optional but recommended [web:451]\n\nmodel.eval()\n\nres = []\n\nfor image_name in tqdm(image_files):\n    # Build a prompt and run inference\n    img_path = \"/kaggle/input/biology-dataset-modified/Biology-Dataset/test\"\n    img = os.path.join(img_path, image_name)\n    image = Image.open(img).convert(\"RGB\")\n    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe this image. Give only one paragraph.\"}]}]\n    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n    \n    inputs = processor(text=prompt, images=[[image]], return_tensors=\"pt\").to(device)\n    with torch.inference_mode():\n        out = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n    caption = processor.batch_decode(out[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n    res.append((image_name, caption))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res.sort(key=lambda x: image_files.index(x[0]))\nfor image, caption in res:\n    print(f\"{image}: {caption}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}